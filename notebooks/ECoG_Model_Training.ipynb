{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RF_L32Ep5MZg"
      },
      "source": [
        "# ECoG Foundation Model Training\n",
        "This is meant to be a minimal notebook which is capable of running model training with a free to use colab notebooks. Feel free to change this as you see fit for your experiments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVNKawOv7ZZ2"
      },
      "outputs": [],
      "source": [
        "# Clone repository.\n",
        "!git clone https://github.com/leoniekerken/ECoG-foundation-model.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGmF6wkX76eD"
      },
      "source": [
        "Now, go into the repo you just downloaded and change the hugging face user access token in the Makefile to your personal access token. If you don't want to do this everytime you could also upload the code to your personal drive and change the path_to_github_repo variable below, although then you risk your code being out of date."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cL5lxjV27x2l"
      },
      "outputs": [],
      "source": [
        "# Download data.\n",
        "!cd ECoG-foundation-model && make download-data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGnG63k1shl_"
      },
      "outputs": [],
      "source": [
        "# Required pip installs.\n",
        "!pip install accelerate\n",
        "!pip install einops\n",
        "!pip install mne\n",
        "!pip install mne-bids\n",
        "!pip install pyEDFlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZqDS43m71Kt"
      },
      "outputs": [],
      "source": [
        "# The local path to the github repo. Must be accessible from this notebook.\n",
        "# If you just run the code above this will work.\n",
        "path_to_github_repo = 'ECoG-foundation-model/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kKJiJx1HFePP"
      },
      "outputs": [],
      "source": [
        "# Add import for ECoG code.\n",
        "import sys\n",
        "import os\n",
        "sys.path.append(os.path.join(path_to_github_repo, 'ECoG_MAE'))\n",
        "\n",
        "# Other imports\n",
        "from dataclasses import dataclass\n",
        "\n",
        "from config import VideoMAEExperimentConfig, VideoMAETaskConfig, ViTConfig, TrainerConfig, ECoGDataConfig\n",
        "from ecog_setup import system_setup, model_setup\n",
        "from loader import dl_setup\n",
        "from train import train_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AItQVYImGnWt"
      },
      "outputs": [],
      "source": [
        "# Configuration for this experiment. See class definition for possible config values and docstrings.\n",
        "experiment_config = VideoMAEExperimentConfig(\n",
        "        video_mae_task_config=VideoMAETaskConfig(\n",
        "            vit_config=ViTConfig(\n",
        "                dim=80,\n",
        "                mlp_dim=80,\n",
        "                patch_size=1,\n",
        "                patch_dims=[1, 1, 1],\n",
        "                frame_patch_size=4,\n",
        "                use_cls_token=False,\n",
        "            ),\n",
        "            tube_mask_ratio=0.5,\n",
        "            decoder_mask_ratio=0.0,\n",
        "            use_contrastive_loss=False,\n",
        "            running_cell_masking=False,\n",
        "        ),\n",
        "        trainer_config=TrainerConfig(\n",
        "            learning_rate=0.0,\n",
        "            num_epochs=10,\n",
        "            loss='patch',\n",
        "        ),\n",
        "        ecog_data_config=ECoGDataConfig(\n",
        "            norm=None,\n",
        "            # I'm not sure on the exact limits but I've managed to\n",
        "            # get a batch size of 32 to work but a batch size of 64 leads to crashes on\n",
        "            # the free tier T4 GPU.\n",
        "            batch_size=8,\n",
        "            data_size=1.0,\n",
        "            env=False,\n",
        "            # bands=[[4, 8], [8, 13], [13, 30], [30, 55], [70, 200]], # You can train over more bands, but encoding data currently only includes high gamma\n",
        "            bands=[[70, 200]],\n",
        "            new_fs=20,\n",
        "            dataset_path=os.path.join(path_to_github_repo, 'dataset'),\n",
        "            train_data_proportion=0.9,\n",
        "            sample_length=2,\n",
        "            shuffle=True,\n",
        "            test_loader=False,\n",
        "        ),\n",
        "        job_name='test_run',\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NmnMkoO-Hj79"
      },
      "outputs": [],
      "source": [
        "accelerator, device, data_type, local_rank = system_setup()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJo9nzuP4y1k"
      },
      "outputs": [],
      "source": [
        "train_dl, test_dl, num_train_samples = dl_setup(experiment_config)\n",
        "\n",
        "# If you want to run a more minimal training run you can uncomment the code\n",
        "# below to limit the number of samples accessible by each dataset. This is\n",
        "# currently inefficient though if you use few training batches because it will\n",
        "# do summaries frequently which takes more time than an individual training\n",
        "# step.\n",
        "# train_num_batches = 1\n",
        "# test_num_batches = 1\n",
        "\n",
        "# for dataset in train_dl.dataset.datasets:\n",
        "#   dataset.max_samples = experiment_config.ecog_data_config.batch_size * train_num_batches\n",
        "# # Can reuse same dataloader for test\n",
        "# test_dl = train_dl\n",
        "# # Or just limit test dataloader\n",
        "# # for dataset in test_dl.dataset.datasets:\n",
        "# #   dataset.max_samples = experiment_config.ecog_data_config.batch_size * test_num_batches\n",
        "\n",
        "# num_train_samples = experiment_config.ecog_data_config.batch_size * train_num_batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6UCfwRrCxa7"
      },
      "outputs": [],
      "source": [
        "# The data is arranged in shape b*c*t*d*h*w, where\n",
        "# b = batch size,\n",
        "# c = freq bands,\n",
        "# t = number of datapoints within a sample (args.new_fs samples per second)\n",
        "# d = depth (currently 1)\n",
        "# h = height of grid (currently 8)\n",
        "# w = width of grid (currently 8)\n",
        "\n",
        "print(next(train_dl._get_iterator()).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NWXEcPHE4uQi"
      },
      "outputs": [],
      "source": [
        "model, optimizer, lr_scheduler, num_patches = model_setup(\n",
        "    experiment_config, device, num_train_samples\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8TWC7OzOkhz"
      },
      "outputs": [],
      "source": [
        "model = train_model(\n",
        "        experiment_config,\n",
        "        device,\n",
        "        model,\n",
        "        train_dl,\n",
        "        test_dl,\n",
        "        num_patches,\n",
        "        optimizer,\n",
        "        lr_scheduler,\n",
        "        accelerator,\n",
        "        data_type,\n",
        "        local_rank,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErfIKIbrLZ4b"
      },
      "source": [
        "You can now view the results of the training in results/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoding"
      ],
      "metadata": {
        "id": "KPykdufdulfI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Ioe5hdQUgzM"
      },
      "outputs": [],
      "source": [
        "from downstream_tasks.encoding.config import EncodingExperimentConfig, EncodingTaskConfig, EncodingDataConfig\n",
        "from downstream_tasks.encoding.utils import run_encoding_task"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoding_experiment_config = EncodingExperimentConfig(\n",
        "    encoding_data_config = EncodingDataConfig(\n",
        "        conversation_data_df_path = os.path.join(path_to_github_repo, \"word-embeddings/gpt2-layer-8-emb.pkl\"),\n",
        "        encoding_neural_data_folder = os.path.join(path_to_github_repo, \"preprocessed-highgamma\"),\n",
        "        electrode_glob_path = \"NY*_*_Part*_conversation*_electrode_preprocess_file_{elec_id}.mat\",\n",
        "        lag = 0\n",
        "    ),\n",
        "    encoding_task_config = EncodingTaskConfig(\n",
        "        model_path = \"\", # Unused here.\n",
        "        embedding_device = \"cuda\",\n",
        "        embedding_batch_size = 8,\n",
        "        num_folds = 2,\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "89iNCpcdvTe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pearson_correlations, mspe = run_encoding_task(encoding_experiment_config, experiment_config.ecog_data_config, model)"
      ],
      "metadata": {
        "id": "Eq5gibh5JSQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pearson_correlations"
      ],
      "metadata": {
        "id": "lpNuTDxtQM5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mspe"
      ],
      "metadata": {
        "id": "xE1VywbKcLM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wtikyMCVcLs6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}