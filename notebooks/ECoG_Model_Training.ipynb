{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RF_L32Ep5MZg"
   },
   "source": [
    "# ECoG Foundation Model Training\n",
    "This is meant to be a minimal notebook which is capable of running model training with a free to use colab notebooks. Feel free to change this as you see fit for your experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repo and data setup (if not already on your machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wVNKawOv7ZZ2"
   },
   "outputs": [],
   "source": [
    "# Clone repository.\n",
    "!git clone https://github.com/leoniekerken/ECoG-foundation-model.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nGmF6wkX76eD"
   },
   "source": [
    "Now, go into the repo you just downloaded and change the hugging face user access token in the Makefile to your personal access token. If you don't want to do this everytime you could also upload the code to your personal drive and change the path_to_github_repo variable below, although then you risk your code being out of date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cL5lxjV27x2l"
   },
   "outputs": [],
   "source": [
    "# Download data.\n",
    "!cd ECoG-foundation-model && make download-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installs and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lGnG63k1shl_"
   },
   "outputs": [],
   "source": [
    "# Required pip installs.\n",
    "!pip install accelerate\n",
    "!pip install einops\n",
    "!pip install mne\n",
    "!pip install mne-bids\n",
    "!pip install pyEDFlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tZqDS43m71Kt"
   },
   "outputs": [],
   "source": [
    "# The local path to the github repo. Must be accessible from this notebook.\n",
    "# If you just run the code above this will work.\n",
    "path_to_github_repo = '../'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kKJiJx1HFePP"
   },
   "outputs": [],
   "source": [
    "# Add import for ECoG code.\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "sys.path.append(os.path.join(path_to_github_repo, 'ECoG_MAE'))\n",
    "\n",
    "# Other imports\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from config import VideoMAEExperimentConfig, VideoMAETaskConfig, ViTConfig, TrainerConfig, ECoGDataConfig\n",
    "import math\n",
    "from ecog_setup import system_setup, model_setup\n",
    "from loader import dl_setup\n",
    "import constants\n",
    "from train import train_model\n",
    "import utils\n",
    "from mae_st_util.models_mae import MaskedAutoencoderViT\n",
    "from plot import plot_multi_band_reconstruction\n",
    "from mask import get_padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AItQVYImGnWt"
   },
   "outputs": [],
   "source": [
    "# Configuration for this experiment. See class definition for possible config values and docstrings.\n",
    "experiment_config = VideoMAEExperimentConfig(\n",
    "        video_mae_task_config=VideoMAETaskConfig(\n",
    "            vit_config=ViTConfig(\n",
    "                # Dimensionality of embeddings through the encoder block.\n",
    "                dim = 64,\n",
    "                # Dimensionality of embeddings through the decoder block.\n",
    "                decoder_embed_dim = 32,\n",
    "                # Ratio of input dimensionality to use as a hidden layer in Transformer Block MLP's\n",
    "                mlp_ratio = 4.0,\n",
    "                # Number of transformer blocks in encoder.\n",
    "                depth = 6,\n",
    "                # Number of transformer blocks in decoder.\n",
    "                decoder_depth = 3,\n",
    "                # Number of attention heads per block in encoder. \n",
    "                num_heads = 8,\n",
    "                # Number of attention heads per block in decoder. \n",
    "                decoder_num_heads = 4,\n",
    "                # Number of electrodes in each patch of input to encoder.\n",
    "                patch_size = 2,\n",
    "                # Number of frames in each patch of input to encoder.\n",
    "                frame_patch_size = 4,\n",
    "                # If true, prepends a cls_token to input to get embedding for classification.\n",
    "                use_cls_token = False,\n",
    "                # If true then use a separate position embedding for the decoder.\n",
    "                sep_pos_embed = True,\n",
    "                # Use truncated normal initialization if True.\n",
    "                trunc_init = False,\n",
    "                # If True then don't use a bias for query, key, and values in attention blocks.\n",
    "                no_qkv_bias = False,\n",
    "            ),\n",
    "            # Proportion of patches to mask out. See MAE as spatio temporal learners paper for details.\n",
    "            encoder_mask_ratio = 0.25,\n",
    "            # Percentage of masks tokens to pass into decoder for reconstruction.\n",
    "            pct_masks_to_decode = 1.0,\n",
    "            # If true then normalize the target before calculating loss. Input is normalized before passing in so likely\n",
    "            # unnecessary.\n",
    "            norm_pix_loss = False,\n",
    "        ),\n",
    "        trainer_config=TrainerConfig(\n",
    "            # Peak learning rate to use in torch OneCyclerLR\n",
    "            max_learning_rate = 5e-4,\n",
    "        ),\n",
    "        ecog_data_config=ECoGDataConfig(\n",
    "            # What percentage of the data to train over.\n",
    "            data_size = 1.0,\n",
    "            # Batch size for training and eval.\n",
    "            batch_size = 32,\n",
    "            # If true then convert data to power envelope by taking magnitude of Hilbert\n",
    "            # transform.\n",
    "            env = False,\n",
    "            # Frequency bands to transform the data into.\n",
    "            bands = [[4, 8], [8, 13], [13, 30], [30, 55], [70, 200]],\n",
    "            # Sample frequency of original dataset.\n",
    "            original_fs = 512,\n",
    "            # Resample rate for new data.\n",
    "            new_fs = 120,\n",
    "            # Relative path to the dataset root directory.\n",
    "            dataset_path = '../dataset',\n",
    "            # Proportion of data to have in training set. The rest will go to test set.\n",
    "            train_data_proportion = 0.9,\n",
    "            # Number of seconds of data to use for a training example.\n",
    "            sample_length = 1,\n",
    "            # If true then shuffle the data before splitting to train and eval.\n",
    "            shuffle = False,\n",
    "        ),\n",
    "        # job_name='test_run',\n",
    "    )\n",
    "\n",
    "# Device to train on.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# Number of training steps to run.\n",
    "max_iters = 10000\n",
    "# Number of validation steps for estimating performance on training and eval data set.\n",
    "eval_iters = 50\n",
    "# How frequently to check performance of validation data set.\n",
    "eval_interval = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl, test_dl, num_train_samples = dl_setup(experiment_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w6UCfwRrCxa7"
   },
   "outputs": [],
   "source": [
    "# The data is arranged in shape b*c*t*d*h*w, where\n",
    "# b = batch size,\n",
    "# c = freq bands,\n",
    "# t = number of datapoints within a sample (args.new_fs samples per second)\n",
    "# h = height of grid (currently 8)\n",
    "# w = width of grid (currently 8)\n",
    "\n",
    "print(next(train_dl._get_iterator()).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = experiment_config.video_mae_task_config.vit_config\n",
    "data_config = experiment_config.ecog_data_config\n",
    "\n",
    "num_frames = experiment_config.ecog_data_config.sample_length * experiment_config.ecog_data_config.new_fs\n",
    "\n",
    "frame_patch_size = model_config.frame_patch_size\n",
    "num_patches = int(  # Defining the number of patches\n",
    "    constants.GRID_SIZE**2\n",
    "    * num_frames\n",
    "    // model_config.patch_size\n",
    "    // frame_patch_size\n",
    ")\n",
    "\n",
    "num_encoder_patches = int(\n",
    "    num_patches * (1 - experiment_config.video_mae_task_config.encoder_mask_ratio)\n",
    ")\n",
    "num_decoder_patches = int(\n",
    "    num_patches * experiment_config.video_mae_task_config.pct_masks_to_decode\n",
    ")\n",
    "print(\"num_patches\", num_patches)\n",
    "print(\"num_encoder_patches\", num_encoder_patches)\n",
    "print(\"num_decoder_patches\", num_decoder_patches)\n",
    "print(\"patch dimensionality\", frame_patch_size * model_config.patch_size * model_config.patch_size * len(data_config.bands))\n",
    "print(\"encoder embedding dimensionality\", model_config.dim)\n",
    "print(\"decoder embedding dimensionality\", model_config.decoder_embed_dim)\n",
    "\n",
    "model = MaskedAutoencoderViT(\n",
    "    img_size=constants.GRID_SIZE,\n",
    "    patch_size=model_config.patch_size,\n",
    "    in_chans=len(experiment_config.ecog_data_config.bands),\n",
    "    embed_dim=model_config.dim,\n",
    "    depth=model_config.depth,\n",
    "    num_heads=model_config.num_heads,\n",
    "    decoder_embed_dim=model_config.decoder_embed_dim,\n",
    "    decoder_depth=model_config.decoder_depth,\n",
    "    decoder_num_heads=model_config.decoder_num_heads,\n",
    "    mlp_ratio=model_config.mlp_ratio,\n",
    "    norm_pix_loss=experiment_config.video_mae_task_config.norm_pix_loss,\n",
    "    num_frames=num_frames,\n",
    "    t_patch_size=model_config.frame_patch_size,\n",
    "    no_qkv_bias=model_config.no_qkv_bias,\n",
    "    sep_pos_embed=model_config.sep_pos_embed,\n",
    "    trunc_init=model_config.trunc_init,\n",
    "    cls_embed=model_config.use_cls_token,\n",
    "    pred_t_dim=num_frames // model_config.frame_patch_size,\n",
    "    img_mask=None,\n",
    "    pct_masks_to_decode=experiment_config.video_mae_task_config.pct_masks_to_decode,\n",
    ")\n",
    "utils.count_params(model)\n",
    "\n",
    "no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "opt_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [\n",
    "            p\n",
    "            for n, p in model.named_parameters()\n",
    "            if not any(nd in n for nd in no_decay)\n",
    "        ],\n",
    "        \"weight_decay\": 1e-2,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [\n",
    "            p\n",
    "            for n, p in model.named_parameters()\n",
    "            if any(nd in n for nd in no_decay)\n",
    "        ],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    opt_grouped_parameters, lr=experiment_config.trainer_config.max_learning_rate\n",
    ")\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=experiment_config.trainer_config.max_learning_rate,\n",
    "    total_steps=max_iters\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Output Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingVisualizer:\n",
    "    def __init__(self):\n",
    "        self.plots = {'train': [], 'val': []}\n",
    "        self.iteration_nums = []\n",
    "        self.losses = {'train': [], 'val': []}\n",
    "        \n",
    "        # Create widgets\n",
    "        self.split_dropdown = widgets.Dropdown(\n",
    "            options=['train', 'val'],\n",
    "            description='Split:',\n",
    "            value='train'\n",
    "        )\n",
    "        \n",
    "        # Instead of a slider, use a dropdown for iterations\n",
    "        self.iter_select = widgets.Dropdown(\n",
    "            options=[],  # Will be populated as we get iterations\n",
    "            description='Iteration:',\n",
    "            value=None\n",
    "        )\n",
    "        \n",
    "        self.output = widgets.Output()\n",
    "        \n",
    "        # Create layout\n",
    "        controls = widgets.VBox([self.split_dropdown, self.iter_select])\n",
    "        self.widget_layout = widgets.VBox([controls, self.output])\n",
    "        display(self.widget_layout)\n",
    "        \n",
    "        # Link widgets\n",
    "        self.interactive = widgets.interactive_output(\n",
    "            self.update_plot,\n",
    "            {'split': self.split_dropdown, 'iteration': self.iter_select}\n",
    "        )\n",
    "    \n",
    "    def update_plot(self, split, iteration):\n",
    "        if not self.plots[split] or iteration is None:  # No plots stored yet\n",
    "            return\n",
    "            \n",
    "        with self.output:\n",
    "            clear_output(wait=True)\n",
    "            idx = self.iteration_nums.index(iteration)\n",
    "            display(self.plots[split][idx])\n",
    "            plt.close()\n",
    "            print(f\"Loss at iteration {iteration}: {self.losses[split][idx]:.4f}\")\n",
    "    \n",
    "    def force_update(self):\n",
    "        \"\"\"Force the widget to update with current values\"\"\"\n",
    "        self.update_plot(self.split_dropdown.value, self.iter_select.value)\n",
    "        time.sleep(0.1)  # Small delay to allow display to update\n",
    "\n",
    "# Create a global visualizer instance\n",
    "visualizer = TrainingVisualizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Loss Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossPlotVisualizer:\n",
    "    def __init__(self, window_size=20):\n",
    "        self.window_size = window_size\n",
    "        self.output = widgets.Output()\n",
    "        display(self.output)\n",
    "        \n",
    "    def update_plot(self, loss_values, lrs):\n",
    "        # Make sure number of loss values is divisible by window size.\n",
    "        trimmed_loss_values = loss_values[:-(len(loss_values) % self.window_size)]\n",
    "        with self.output:\n",
    "            clear_output(wait=True)\n",
    "            fig, (ax1, ax2) = plt.subplots(nrows=2, sharex=True)\n",
    "            fig.set_figwidth(10)\n",
    "            fig.set_figheight(8)\n",
    "            averaged_loss = torch.tensor(trimmed_loss_values).view(-1, self.window_size).mean(1)\n",
    "            ax1.plot([i * self.window_size for i in range(len(averaged_loss))], averaged_loss)\n",
    "            ax1.set_title('Training Loss')\n",
    "            ax1.set_xlabel('Steps (averaged over {} iterations)'.format(self.window_size))\n",
    "            ax1.set_ylabel('Loss')\n",
    "            ax1.grid(True)\n",
    "            ax2.plot(lrs)\n",
    "            ax2.set_title('Learning Rate')\n",
    "            ax2.set_xlabel('Steps'.format(self.window_size))\n",
    "            ax2.set_ylabel('Learning Rate')\n",
    "            ax2.grid(True)\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "\n",
    "# Create the loss visualizer\n",
    "loss_visualizer = LossPlotVisualizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Util Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            batch = get_batch(split)\n",
    "            padding_mask = get_padding_mask(batch, device)\n",
    "            model.initialize_mask(padding_mask)\n",
    "            batch = torch.nan_to_num(batch)\n",
    "            loss, pred, mask, latent, correlation = model(batch, mask_ratio=experiment_config.video_mae_task_config.encoder_mask_ratio)\n",
    "            if k == 0:  # Only store the first reconstruction of each evaluation\n",
    "                norm_batch = model.forward_input_norm(batch)\n",
    "                pred_signal = model.unpatchify(pred)\n",
    "                pred_signal_np = pred_signal.detach().cpu().numpy()\n",
    "                masked_signal = apply_mask_to_batch(model, norm_batch, mask)\n",
    "                fig = plot_multi_band_reconstruction(\n",
    "                    norm_batch.detach().cpu().numpy(),\n",
    "                    pred_signal_np,\n",
    "                    experiment_config.video_mae_task_config.vit_config.frame_patch_size,\n",
    "                    seen_signal=masked_signal.detach().cpu().numpy()\n",
    "                )\n",
    "                # Store the plot\n",
    "                visualizer.plots[split].append(fig)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "        # Store the loss\n",
    "        visualizer.losses[split].append(out[split])\n",
    "        \n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    dataloader = train_dl if split == \"train\" else test_dl\n",
    "    return next(enumerate(dataloader))[1]\n",
    "\n",
    "def apply_mask_to_batch(model, batch, mask):\n",
    "    \"\"\"Replaces values in batch with nan when mask is False.\n",
    "    model: VideoViT\n",
    "    mask: (batch_size, frames // frame_patch_size)\n",
    "    batch: (batch_size, num_channels, frames, electrode_height, electrode_width)\n",
    "    \"\"\"\n",
    "    batch_patch = model.patchify(batch)\n",
    "    # Mask is for every frame_patch_size frames, so expand to align with batch\n",
    "    # tensor and fill in masked out information with nan.\n",
    "    B, t = mask.shape\n",
    "    _, T, P = batch_patch.shape\n",
    "    frame_patch_size = T // t\n",
    "    # Repeats mask values to align with batch dimensions.\n",
    "    mask = mask.repeat_interleave(frame_patch_size * P, axis=1).view(B, T, P).to(torch.bool)\n",
    "    return model.unpatchify(batch_patch.masked_fill(mask, torch.nan))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j8TWC7OzOkhz"
   },
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "print(sum(p.numel() for p in model.parameters()) / 1e6, \"M parameters\")\n",
    "loss_i = []\n",
    "lr_i = []\n",
    "\n",
    "# batch = get_batch(\"train\")\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        visualizer.iteration_nums.append(iter)\n",
    "        visualizer.iter_select.options = visualizer.iteration_nums\n",
    "        visualizer.iter_select.value = iter\n",
    "        \n",
    "        losses = estimate_loss()\n",
    "        print(\n",
    "            f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\"\n",
    "        )\n",
    "        visualizer.force_update()\n",
    "        loss_visualizer.update_plot(loss_i, lr_i)\n",
    "\n",
    "    batch = get_batch(\"train\")\n",
    "    signal = batch.to(device)\n",
    "    padding_mask = get_padding_mask(signal, device)\n",
    "    signal = torch.nan_to_num(signal)\n",
    "    model.initialize_mask(padding_mask)\n",
    "    \n",
    "    loss, pred, mask, latent, correlation = model(signal, mask_ratio=experiment_config.video_mae_task_config.encoder_mask_ratio)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    lr_scheduler.step()\n",
    "    \n",
    "    loss_i.append(loss.item())\n",
    "    lr_i.append(lr_scheduler.get_lr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KPykdufdulfI",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Ioe5hdQUgzM"
   },
   "outputs": [],
   "source": [
    "from downstream_tasks.encoding_decoding.config import EncodingDecodingExperimentConfig, EncodingDecodingTaskConfig, EncodingDecodingDataConfig\n",
    "from downstream_tasks.encoding_decoding.utils import run_encoding_task, run_decoding_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "89iNCpcdvTe8"
   },
   "outputs": [],
   "source": [
    "encoding_experiment_config = EncodingDecodingExperimentConfig(\n",
    "    encoding_data_config = EncodingDecodingDataConfig(\n",
    "        conversation_data_df_path = os.path.join(path_to_github_repo, \"word-embeddings/gpt2-layer-8-emb.pkl\"),\n",
    "        encoding_neural_data_folder = os.path.join(path_to_github_repo, \"preprocessed-highgamma\"),\n",
    "        electrode_glob_path = \"NY*_*_Part*_conversation*_electrode_preprocess_file_{elec_id}.mat\",\n",
    "        lag = 0\n",
    "    ),\n",
    "    encoding_task_config = EncodingDecodingTaskConfig(\n",
    "        model_path = \"\", # Unused here.\n",
    "        embedding_device = \"cuda\",\n",
    "        embedding_batch_size = 8,\n",
    "        num_folds = 2,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eq5gibh5JSQy"
   },
   "outputs": [],
   "source": [
    "pearson_correlations, mspe = run_encoding_task(encoding_experiment_config, experiment_config.ecog_data_config, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lpNuTDxtQM5G"
   },
   "outputs": [],
   "source": [
    "pearson_correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xE1VywbKcLM9"
   },
   "outputs": [],
   "source": [
    "mspe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wtikyMCVcLs6"
   },
   "outputs": [],
   "source": [
    "pearson_correlations, mspe = run_encoding_task(encoding_experiment_config, experiment_config.ecog_data_config, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearson_correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mspe"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
